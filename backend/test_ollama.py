"""Test script for Ollama LLaMA wrapper - optimal for clinical EMR summarization."""

import sys
import os

# Add backend to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

def test_ollama():
    try:
        from ollama_wrapper import generate_json_summary, model_info
        
        print("=== Ollama Clinical Test ===")
        print("Configuration:")
        info = model_info()
        for key, value in info.items():
            print(f"  {key}: {value}")
        print()
        
        if not info.get("running", False):
            print("‚ùå Ollama not running!")
            print("\nSetup steps:")
            print("  1. Download: https://ollama.ai")
            print("  2. Start service: ollama serve")
            print("  3. Download model: ollama pull llama2:7b-chat")
            print("  4. Alternative models:")
            print("     - ollama pull mistral:7b")
            print("     - ollama pull codellama:7b")
            return False
        
        # Test with realistic clinical data
        clinical_notes = [
            {
                "note_id": 1,
                "patient_id": 1,
                "note_date": "2025-01-15",
                "snippet": "Patient presents with persistent headaches, BP elevated at 150/90. Started on lisinopril 10mg daily. Patient reports good medication compliance.",
                "score": 0.95
            },
            {
                "note_id": 2,
                "patient_id": 1,
                "note_date": "2025-01-12",
                "snippet": "Follow-up visit: Morning headaches continue but less severe. Blood pressure improving on current regimen. Patient tolerat```ing medication well.",
                "score": 0.88
            },
            {
                "note_id": 3,
                "patient_id": 1,
                "note_date": "2025-01-08",
                "snippet": "Initial complaint: Daily headaches for 2 weeks, family history of hypertension. Vital signs: BP 145/88, otherwise normal exam.",
                "score": 0.82
            }
        ]
        
        print("Testing with clinical notes:")
        for note in clinical_notes:
            print(f"  Note {note['note_id']} ({note['note_date']}): {note['snippet'][:60]}...")
        print()
        
        print("Generating summary... (may take 5-15 seconds)")
        summary = generate_json_summary(clinical_notes, "What is the patient's blood pressure trend?")
        
        print("‚úÖ Generation successful!")
        print("\nClinical Summary:")
        print(f"  One-line: {summary.get('one_line')}")
        print("  Key findings:")
        for i, bullet in enumerate(summary.get('bullets', []), 1):
            print(f"    {i}. {bullet}")
        print(f"  Sources: {len(summary.get('sources', []))} notes")
        print(f"  Generated by: {summary.get('source', 'ollama')}")
        
        return True
        
    except ImportError as e:
        print(f"‚ùå Import error: {e}")
        print("\nOllama wrapper available but Ollama not installed.")
        return False
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        print(f"Error type: {type(e).__name__}")
        
        # Troubleshooting info
        try:
            from ollama_wrapper import model_info
            info = model_info()
            if not info.get("running"):
                print("\nüîß Troubleshooting:")
                print("  - Ensure Ollama is running: ollama serve")
                print("  - Check available models: ollama list")
                print("  - Download LLaMA: ollama pull llama2:7b-chat")
        except Exception:
            pass
            
        return False


if __name__ == "__main__":
    print("Ollama LLaMA Clinical Test")
    print("=" * 50)
    
    success = test_ollama()
    
    if success:
        print("\nüéâ Ollama test successful!")
        print("\nYour EMR now uses this priority order:")
        print("  1. üè• Cohere (cloud, fast)")
        print("  2. üß† Ollama LLaMA-7B (local, high quality)")  
        print("  3. ü§ó Hugging Face (cloud)")
        print("  4. üì¶ GPT4All (local, smaller)")
        print("  5. üîÑ Mock summary (instant fallback)")
        print("\nüí° Ollama provides the best balance of quality and privacy!")
    else:
        print("\n‚ö†Ô∏è  Ollama setup needed")
        print("\nQuick start:")
        print("  1. Visit: https://ollama.ai")
        print("  2. Download and install")
        print("  3. Run: ollama serve")
        print("  4. Run: ollama pull llama2:7b-chat")
        print("  5. Test: python test_ollama.py")